{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import urllib3\n",
    "import csv\n",
    "import logging\n",
    "import re\n",
    "import uuid\n",
    "import shortuuid\n",
    "from pathlib import Path\n",
    "from idmc_utils import generate_taskflow, package_import\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Set the pandas options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the log file\n",
    "logging.basicConfig(\n",
    "    filename='logs/console.log',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the config file\n",
    "logging.info('Reading the config file')\n",
    "with open('config/config.json', 'r') as infile:\n",
    "    config = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the execution plans\n",
    "logging.info('Reading the execution plans')\n",
    "dfPlans = pd.read_csv('in/plans.csv', dtype='str', encoding='utf-8', na_filter=False)\n",
    "dfPlans['plan_step_order'] = dfPlans['plan_step_order'].astype(int)\n",
    "\n",
    "#TODO remove the below filter once done with testing\n",
    "test_plans = [\n",
    "    'Echo Employee Snapshot Oracle R12.1.3', \n",
    "    'Echo GL Balance Refresh', \n",
    "    'Echo Account Dimension', \n",
    "    'Human Resources - Oracle R1213 - Flexfield', \n",
    "    'Common-LoadDayDimension Universal',\n",
    "    'Process_Quality_OracleR1213'\n",
    "    #'Financials_Universal'\n",
    "    ]\n",
    "dfPlans = dfPlans[dfPlans['plan_name'].isin(test_plans)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parameter values file\n",
    "dfParamVals = pd.read_csv('in/param_values.csv', dtype='str', encoding='utf-8', na_filter=False)\n",
    "dfParamVals['sequence'] = dfParamVals['sequence'].astype(float)\n",
    "dfParamVals = dfParamVals.sort_values(['obj_wid', 'sequence'])\n",
    "\n",
    "# Read the task parameter files\n",
    "dfTaskParams = pd.read_csv('in/params-task.csv', dtype='str', encoding='utf-8', na_filter=False)\n",
    "dfTaskParams = dfTaskParams.merge(dfPlans, how='inner', on='step_wid')\n",
    "dfTaskParams = dfTaskParams[['name', 'value', 'type_cd', 'step_wid', 'datatype', 'context_type',\n",
    "       'inactive_flag', 'comments', 'plan_name', 'step_name', 'param_wid']]\n",
    "dfTaskParams = dfTaskParams.drop_duplicates()\n",
    "dfTaskParams['param_filename'] = dfTaskParams['plan_name'].apply(lambda x: re.sub(r'\\W+', '_', x) + '.param')\n",
    "\n",
    "# String join the multi line values\n",
    "dfParamVals = dfParamVals.groupby(['obj_wid','obj_type'])\n",
    "dfParamVals = dfParamVals['textfield'].apply(lambda x: ''.join(x))\n",
    "dfParamVals = pd.DataFrame(dfParamVals)\n",
    "dfParamVals = dfParamVals.reset_index()\n",
    "dfParamVals['param_values'] = dfParamVals['textfield'].str.split('_dac_sep_')\n",
    "dfParamVals = dfParamVals.explode('param_values')\n",
    "dfParamVals['value_type'] = dfParamVals['param_values'].str.extract(r'^(\\S+)=')\n",
    "dfParamVals['value'] = dfParamVals['param_values'].str.extract(r'^\\S+=(.*)$')\n",
    "dfParamVals = dfParamVals[~dfParamVals['value_type'].isna()]\n",
    "dfParamVals = dfParamVals.reset_index(drop=True)\n",
    "dfParamVals = dfParamVals.pivot(index=['obj_wid', 'obj_type', 'textfield'], columns=['value_type'], values=['value'])\n",
    "dfParamVals = dfParamVals.reset_index()\n",
    "dfParamVals.columns = ['_'.join(col).strip() for col in dfParamVals.columns.values]\n",
    "dfParamVals['is_static'] = dfParamVals['value_Runtime'].apply(lambda x: False if x == 'true' else True)\n",
    "dfParamVals = dfParamVals.rename(columns={'obj_wid_': 'param_wid', 'obj_type_': 'param_type', 'textfield_': 'raw_text'})\n",
    "\n",
    "# Join the values onto the params\n",
    "dfTaskParams = dfTaskParams.merge(dfParamVals, how='left', on='param_wid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the execution parameter files\n",
    "dfExecParams = pd.read_csv('in/params-exec.csv', dtype='str', encoding='utf-8', na_filter=False)\n",
    "dfExecParams = dfExecParams.merge(dfPlans, how='inner', on='plan_wid')\n",
    "dfExecParams = dfExecParams[['param_wid', 'name', 'type_cd', 'plan_wid', 'datatype', 'context_type',\n",
    "       'inactive_flag', 'comments', 'plan_name']]\n",
    "dfExecParams = dfExecParams.drop_duplicates()\n",
    "dfExecParams['param_filename'] = dfExecParams['plan_name'].apply(lambda x: re.sub(r'\\W+', '_', x) + '.param')\n",
    "dfExecParams['name'] = dfExecParams['name'].apply(lambda x: x if x.startswith('$$') else '$$' + x if len(x) > 0 else x)\n",
    "\n",
    "# Join the values onto the params\n",
    "dfExecParams = dfExecParams.merge(dfParamVals, how='left', on='param_wid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookup the Converted Mapping Task IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to IDMC\n",
    "logging.info('Logging into IDMC')\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "data = '{ \"username\": \"' + config['idmc']['user'] + '\", \"password\": \"' + config['idmc']['password'] + '\" }'\n",
    "\n",
    "url = 'https://' + config['idmc']['host'] + '/saas/public/core/v3/login'\n",
    "r = http.request(\n",
    "    'POST', \n",
    "    url,\n",
    "    timeout=3000,\n",
    "    body=data,\n",
    "    headers={\n",
    "            'Accept': 'application/json',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "# Convert the response into a datframe\n",
    "result = json.loads(r.data.decode('utf-8'))\n",
    "sessionID = result['userInfo']['sessionId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the secure agent ID\n",
    "logging.info('Getting the secure agent ID')\n",
    "url = 'https://' + config['idmc']['pod'] + '.' + config['idmc']['host'] + '/saas/api/v2/runtimeEnvironment/name/' + quote(config['idmc']['agentGroupName'])\n",
    "r = http.request(\n",
    "    'GET', \n",
    "    url,\n",
    "    timeout=3000,\n",
    "    headers={\n",
    "            'Accept': 'application/json',\n",
    "            'icSessionId': sessionID\n",
    "        }\n",
    "    )\n",
    "    \n",
    "# Convert the response into a datframe\n",
    "result = json.loads(r.data.decode('utf-8'))\n",
    "agentGroupID = result['id']\n",
    "agentGroupGUID = result['federatedId']\n",
    "agentGroupName = config['idmc']['agentGroupName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping. dfCount = 200; i = 200; taskCount = 465\n",
      "Looping. dfCount = 400; i = 400; taskCount = 465\n",
      "Finished. dfCount = 465; i = 600; taskCount = 465\n"
     ]
    }
   ],
   "source": [
    "# Initialise the tasks data frame\n",
    "logging.info('Getting a list of the mapping tasks')\n",
    "limit = 200\n",
    "i = 0\n",
    "dfTasks = pd.DataFrame()\n",
    "\n",
    "# Page through mapping task queries\n",
    "while True:\n",
    "\n",
    "    # Get a list of the mapping tasks\n",
    "    url = 'https://' + config['idmc']['pod'] + '.' + config['idmc']['host'] + '/saas/public/core/v3/objects?q=type==%27MTT%27%20and%20location==%27' + config['idmc']['folderPath'] + '%27&limit=' + str(limit) + '&skip=' + str(i)\n",
    "    r = http.request(\n",
    "        'GET', \n",
    "        url,\n",
    "        timeout=3000,\n",
    "        headers={\n",
    "                'Accept': 'application/json',\n",
    "                'INFA-SESSION-ID': sessionID\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    # Convert the response into a datframe\n",
    "    result = json.loads(r.data.decode('utf-8'))\n",
    "    dfTmp = pd.json_normalize(result)\n",
    "    dfResp = dfTmp.copy()\n",
    "    taskCount = dfResp.iloc[0]['count']\n",
    "    dfResp = dfResp['objects'].explode()\n",
    "    dfResp = pd.DataFrame(dfResp)\n",
    "    dfResp = pd.json_normalize(dfResp['objects'])\n",
    "    dfTasks = pd.concat([dfTasks, dfResp], ignore_index=True)\n",
    "\n",
    "    # Break if all records have been returned\n",
    "    i = i + limit\n",
    "    if i > taskCount:\n",
    "        print(f'Finished. dfCount = { len(dfTasks) }; i = { i }; taskCount = { taskCount }')\n",
    "        break\n",
    "\n",
    "    print(f'Looping. dfCount = { len(dfTasks) }; i = { i }; taskCount = { taskCount }')\n",
    "\n",
    "# Clean up the list of returned tasks\n",
    "dfTasks = dfTasks.drop(columns=['tags'])\n",
    "dfTasks = dfTasks.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the IDMC info onto the plans\n",
    "dfTasks['step_name'] = dfTasks['path'].apply(lambda x: os.path.basename(x))\n",
    "dfTasks = dfTasks[['step_name','id','path']]\n",
    "dfTasks = dfTasks.rename(columns={'id': 'infa_id', 'path': 'infa_path'})\n",
    "dfPlans = dfPlans.merge(dfTasks, how='left', on='step_name')\n",
    "dfPlans['agent_id'] = agentGroupID\n",
    "dfPlans['agent_guid'] = agentGroupGUID\n",
    "dfPlans['agent_name'] = agentGroupName\n",
    "dfPlans['script_dir'] = config['local']['scriptsDir']\n",
    "dfPlans['script_args'] = '' # TODO placeholder for any args that need to be passed to the step script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log an error if any plans did not find an existing matching task\n",
    "dfMissing = dfPlans[(dfPlans['infa_id'] == '') | (dfPlans['infa_id'].isna())].copy()\n",
    "if len(dfMissing.index) > 0:\n",
    "    logging.error('Some plans are missing a converted mapping task. Please see \"out/missing_tasks.csv\" for more details')\n",
    "    dfMissing.to_csv('out/missing_tasks.csv', index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Parameter Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Echo_Employee_Snapshot_Oracle_R12_1_3.param', 'Echo_Employee_Snapshot_Oracle_R12_1_3.param', 'Echo_Employee_Snapshot_Oracle_R12_1_3.param', 'Echo_Employee_Snapshot_Oracle_R12_1_3.param']\n"
     ]
    }
   ],
   "source": [
    "logging.info('Generating the parameter files...')\n",
    "task_files = dfTaskParams['param_filename'].to_list()\n",
    "exec_files = dfExecParams['param_filename'].to_list()\n",
    "combined_files = task_files + exec_files\n",
    "print(combined_files)\n",
    "unique_files = list(set(combined_files))\n",
    "\n",
    "for filename in unique_files:\n",
    "    \n",
    "    logging.info(f'Generating the parameter file \"out/{ filename }\"...')\n",
    "    unique_sections = dfTaskParams[dfTaskParams['param_filename'] == filename]['step_name'].unique()\n",
    "\n",
    "    # Set the param file header\n",
    "    lines = ['#USE_SECTIONS', '']\n",
    "\n",
    "    for section in unique_sections:\n",
    "\n",
    "        dfSection = dfTaskParams[( dfTaskParams['param_filename'] == filename ) & ( dfTaskParams['step_name'] == section )].copy()\n",
    "\n",
    "        # Append the section header\n",
    "        step_name = dfSection.iloc[0][\"step_name\"]\n",
    "        lines.append(f'[Default].[{ step_name }]')\n",
    "\n",
    "        # Add the parameter values\n",
    "        for idx, row  in dfSection.iterrows():\n",
    "            param_name = row['name']\n",
    "            if row['is_static'] and row['datatype'] == 'Text':\n",
    "                param_value = row['value_Text']\n",
    "            elif row['is_static'] and row['datatype'] == 'Timestamp' and row['value_Function'] == 'TO_CUSTOM':\n",
    "                #TODO add date conversion using the format\n",
    "                param_format = row['value_Formatter']\n",
    "                param_value = row['value_Value']\n",
    "            elif row['datatype'] == 'SQL':\n",
    "                param_value = row['value_SQL']\n",
    "            else:\n",
    "                param_value = row['value']\n",
    "            lines.append(f\"{ param_name }={ param_value }\")\n",
    "        lines.append('')\n",
    "\n",
    "    # Add the global footer\n",
    "    lines.append('[Global]')\n",
    "    dfGlobal = dfExecParams[dfExecParams['param_filename'] == filename].copy()\n",
    "    for idx, row in dfGlobal.iterrows():\n",
    "        param_name = row['name']\n",
    "        if row['is_static'] and row['datatype'] == 'Text':\n",
    "            param_value = row['value_Text']\n",
    "        elif row['is_static'] and row['datatype'] == 'Timestamp' and row['value_Function'] == 'TO_CUSTOM':\n",
    "            #TODO add date conversion using the format\n",
    "            param_format = row['value_Formatter']\n",
    "            param_value = row['value_Value']\n",
    "        elif row['datatype'] == 'SQL':\n",
    "            param_value = row['value_SQL']\n",
    "        else:\n",
    "            param_value = row['value']\n",
    "        lines.append(f\"{ param_name }={ param_value }\")\n",
    "\n",
    "    lines.append('')\n",
    "\n",
    "    with open(f'out/{ filename }', 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Taskflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Generating the taskflows...')\n",
    "\n",
    "# Filter out any missing tasks\n",
    "dfPlansFil = dfPlans.copy()\n",
    "dfPlansFil = dfPlansFil[(dfPlansFil['infa_id'] != '') & (dfPlansFil['infa_id'].notna())]\n",
    "\n",
    "#TODO Remove the below filter after testing is finished\n",
    "#dfPlansFil = dfPlansFil[dfPlansFil['step_name']== 'SDE_Universal_APTermsDimension']\n",
    "\n",
    "planIds = dfPlansFil['plan_wid'].unique()\n",
    "\n",
    "for planId in planIds:\n",
    "\n",
    "    # Get the plan for the current id\n",
    "    dfPlan = dfPlansFil[dfPlansFil['plan_wid'] == planId].copy()\n",
    "\n",
    "    # Generate the unique identifiers\n",
    "    dfPlan['dac2idmc_step_id'] = dfPlan.apply(lambda x: \"task\" + shortuuid.uuid()[:8], axis=1)\n",
    "    map_order_uuid = { step_order: \"group\" + shortuuid.uuid()[:8] for step_order in dfPlan['plan_step_order'].unique() }\n",
    "    dfPlan = dfPlan.sort_values(['plan_wid', 'plan_step_order', 'step_name'])\n",
    "    dfPlan['sequence'] = range(len(dfPlan))\n",
    "    dfPlan['dac2idmc_group_id'] = dfPlan['plan_step_order'].map(map_order_uuid)\n",
    "    dfPlan['dac2idmc_next_id'] = dfPlan['dac2idmc_step_id'].shift(-1)\n",
    "\n",
    "    # Get the next group IDs and sort the plans\n",
    "    dfGroups = dfPlan[['plan_step_order','dac2idmc_group_id']].copy()\n",
    "    dfGroups = dfGroups.drop_duplicates()\n",
    "    dfGroups = dfGroups.sort_values(['plan_step_order'])\n",
    "    dfGroups['dac2idmc_next_group'] = dfGroups['dac2idmc_group_id'].shift(-1)\n",
    "    dfGroups = dfGroups[['dac2idmc_group_id','dac2idmc_next_group']]\n",
    "\n",
    "    dfPlan = dfPlan.merge(dfGroups, how='inner', on='dac2idmc_group_id')\n",
    "    dfPlan = dfPlan.sort_values(['sequence'])\n",
    "\n",
    "    #TODO Uncomment the below lines for troubleshooting\n",
    "    #print(planId)\n",
    "    #dfPlan.to_excel(f'out/{ planId }.xlsx')\n",
    "\n",
    "    # Generate the taskflow ID\n",
    "    taskflowID = shortuuid.uuid()\n",
    "    taskflowName = dfPlan.iloc[0]['plan_name']\n",
    "    taskflowName = re.sub(r'\\W+', '_', taskflowName)\n",
    "\n",
    "    logging.info(f'Create workspace tree \"tmp/{ taskflowName }/Explore/Default\"')\n",
    "\n",
    "    # Create the workspace directories\n",
    "    treePath = Path(f'tmp/{ taskflowName }/Explore/Default')\n",
    "    treePath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logging.info(f'Generating taskflow \"{ taskflowName }\"')\n",
    "    generate_taskflow.generate_taskflow(taskflowID, taskflowName, dfPlan, config)\n",
    "\n",
    "    logging.info(f'Packaging taskflow \"{ taskflowName }\"')\n",
    "    package_import.package_import(taskflowID, taskflowName, dfPlan)\n",
    "\n",
    "    logging.info(f'Done!')\n",
    "\n",
    "logging.info('All taskflows have been generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing below this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Echo_Employee_Snapshot_Oracle_R12_1_3.param']"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Echo_Employee_Snapshot_Oracle_R12_1_3.param',\n",
       " 'Echo_Employee_Snapshot_Oracle_R12_1_3.param',\n",
       " 'Echo_Employee_Snapshot_Oracle_R12_1_3.param',\n",
       " 'Echo_Employee_Snapshot_Oracle_R12_1_3.param']"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Echo_Employee_Snapshot_Oracle_R12_1_3.param'], dtype=object)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTaskParams['param_filename'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Echo_Employee_Snapshot_Oracle_R12_1_3.param'], dtype=object)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfExecParams['param_filename'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
